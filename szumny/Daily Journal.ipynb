{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 1 (05/30-06/2)   \n",
    "                          \n",
    "Day 1 (Tuesday, May 30th, 2023): Today was my first day working at Argonne. I was pretty nervous to start but after going through my day I calmed down. The day started off with attending orientation from 9-12, and then lunch from 12-1. It was then time to report to my workstation and meet with my mentors and supervisor. It was really great to meet them, and they gave me a rundown of my project and what to expect from it. I didn’t do much actual work, but I spent the rest of the day completing all my required training, and also set up my computer to be able to code.\n",
    "\n",
    "Day 2 (Wednesday, May 31st, 2023): Today started off by once again attending orientation from 9-12. I went to lunch yet again, and at 1 reported back to my workstation. I had to complete the rest of my training as they had added some for me to do. I then began to start working on my project. My first task was to convert all of the xml files from the given data into one large csv file, with information such as the filename, the folder, and the coordinates of the smoke. I worked on this all day, but got stuck on figuring out how to get the coordinates into their correct spots. The reason for this was because the direct child of the object in the xml file was not multiple x’s and y’s, but one large text which I would have to split up in order to get the respective x’s and y’s.\n",
    "\n",
    "Day 3 (Thursday, June 1st, 2023): Today was my first full day at work. I got to my station at 8:30, and began by checking all my emails and making sure I'm up to date with everything. I then created my personal introduction chart, and then I began trying to crack down on the problem. I managed to figure out how to successfully get the maximum and minimum of the x’s and y’s into their respective dataframe locations. I did this by looping through the child object and comparing them to preset variables in order to get the absolute max and absolute min. After I completed this, I went over to the weekly meeting at 10:00, in which we all introduced ourselves, and asked any question that we may have had. I created a different function, which would iterate through every folder and file of xml’s from the data, run each of those xml files through the single xml to csv file function which I created. In the code it would create each individual dataframe, and then concat or combine them all into one large dataframe. I was very proud after I completed this step, because it felt like it took forever. After this I went to lunch until 1:00. Once I came back, I started trying to figure out how to crop each image. My next step was to take every instance of smoke from the csv file I created, and crop it at the given points to only have pictures of the actual smoke. Bhupendra told me to first only crop one individual image, in order to see how it crops. I used the Pillow Image functions in order to do this, but once I cropped the image I realized that something was wrong. It didn’t crop only the smoke and instead cropped the image randomly. I thought about why this may be and after looking through my code I realized that when I actually converted the xml to the df, I was grabbing the points of the image, and not the points of the boundary boxes of the smoke. I went back to fix my mistake, and once I had gone through my other code to fix and other errors that arose, the single image crop worked perfectly. I then wrote a function which would take the csv file of size 9,550, and crop each image in the dataset to their smoke coordinates. This function was much easier to create, but when I was running it (which took a very long time because I was saving 9,500 cropped images into a separate folder on my computer), the function kept crashing at one given moment. After looking at the error message which said the given directory was not found, I double checked and found that there was actually a typo in the xml file for the folder name, where it contained a dash and not an underscore. I added a condition in the code where if that specific folder was found, to change the name to its correct name, which worked. This error happened a few times while running the code, but I managed to correct all of the discrepancies and the function finished running. I now have a complete file with all images of the smoke which were cropped.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 4 (Friday, June 2nd, 2023): I first began by trying to figure out how to resize my images. I had to think about this for a while because this would be very important later on. I did some analyzing of the image sizes from my cropped images, and found that a majority of them were at least 100 by 100. So I plan on going in that direction. I started exploring a way to split the images further, specifically by rgb color ratios, but later after talking with bhupendra, I will do it simpler now and can go down that path later on. My next goal was to get about the same amount of sky images and ground images as my smoke imagees (about 9000). I was pretty stuck on this, and was trying out many things. I first tried to reverse crop the images, to remove the smoke and leave everything else. I worked on this and when I was done, I realized there was too much error, where a lot of smoke was still ending up in the image. I decided to go down a different road, and to figure out how to crop the images better. I created a program, which would check if the beginning of the smoke was before or after the middle of that image. If it was before, I would crop the full image from the max x of the smoke to the full size of the image, and if it was after the middle, I would crop the image from 0 to the min x of the smoke. I kept y as the full image, as I want to be able to use the sky and the ground. I tested it, but having it start right at the start or end of smoke also had a little bit of smoke poking through, so I then respectively pushed it back by about 250 pixels, which seemed to do the trick. At this point I was cropping the images well to not have smoke, but now I had to seperate the sky and the ground. I once again cropped the image I got in the same for loop, and changed the y this time, while keeping the full x of the cropped image. I changed y by a few different values with trial and error, but then found that 1050, was a pretty good height to seperate the ground and the sky. Once I ran this code, I was left with 2 new folders containing either sky or ground pictures. These pictures were very large, and I recieved advice to split the images up to have even more images. This was a tough one, but I found a really good function called image_slicer, which could slice an image and save the new images automatically with a given input of the amount of slices. I also did a bit of trial and error with the amount of slices, but I found that 9 keeps the images mostly above 100 by 100, and had pretty solid results. I ran this program and it worked perfectly as it saved about 9,000 new sliced images into new files, ground slices and sky slices respectively. This now leaves me with a folder of 9,000 smoke images, and 9,000 sky and ground images as well. This will allow me to obtain better results when I begin creating a model. Starting to create this model will be my first task starting next week."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 2 (06/05 - 06/09)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1 (Monday, June 6th, 2023): Since I managed to get all of the necessary pictures last Friday for the dataset, today I was able to actually go and begin starting to train the VGG16 model with my dataset. First, I had to resize all of the images to be able to be used by the VGG16 model. This size is 224x224, and currently my images were all around the place. Also, in order to convert the images into the data that the dataset could use, I needed to put them in a new folder, and the folder names were actually the classes. I did this all in one step, by creating code which would take all of the pictures, and resize them and put those copies into the new folders. The folders actually also had to be seperated by training and testing data, so I incorporated a random event of 80% and 20%, and if it was 80% I would resize the image and send it to the training file, and otherwise send it to the testing file. This way the pictures would be randomly spread out, leading to better results. I found a few different websites explaining how to use the VGG16 model, so I began to try and implement it. There ws a lot of trial and error, as at first my images weren't being converted, and then later the model was not accepting my data. In the end I managed to have everything work, and began to train the model. The issue with this, is that it takes incredibly long to do so. My first attempt I waited for a very long time, just to realize that it was only for the first epoch, and there were 99 more to go. This would not work, so I managed to reduce the computational time by almost 40 minutes for each epoch, and also reduced the amount of steps per epoch. Then there was more trial and error. The model would refuse to run, and it turned out it was an issue with the way I installed keras. Once I fixed that, my checkpoint was not working. The checkpoint is very imporatnt, as it allows you to run the training for a very long time, and stop and continue whenever you'd like, as it takes the most accurate epoch after every one, and saves it to a file on my computer. I finally figured out how to save it after each epoch. Now I am just letting it train, and hopefully tomorrow I will be able to test the data on my first attempt at the model. Currently it is at about a 65% accuracy, which is incredibly good for the first attempt. Only one issue exists and it is that when I try and start the model at the last best saved weights, it has 19 insteead of 16 layers, which I will have to figure out tomorrow.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 2 (Tuesday, June 7th, 2023): I tried to figure out how to load the data, but nothing seemed to work. I ran it a few times, but it also was a very big program, so it also kept on crashing. I tried everything, but Seongha recommended I use the PyTorch library instead of TensorFlow. She said it was a little easier to use, and see where it goes wrong. So I found a few PyTorch VGG16 models online, and tried to implement them. This forced me to have to switch up the way I stored my files, in order for the model to be easily able to read the data. I basically just added another folder with validation data, and did this on a 70, 20, 10 split between training, validation, and testing, respectively. After that I was able to start writing the model. Once I was done, I could already tell that this new model worked much better than before. A new issue ended up arising, where my computer does not have a GPU and low memory, so I am not able to train my model on it. Sean recommended that I use Google Colab, as the base free version allows for 12 GB of memory, and a decent GPU. I am now currently working on transferring the model over to there, and my data, in order to train it on that instead. Hopefully tomorrow I will be able to finally complete the training of the model, and be able to test my first VGG16 model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 3 (Wednesday, June 8th, 2023): After my meeting in the morning, I started to upload my data to Google Colab. This ended up taking a very long time, because of how many images were in the dataset. What I ended up figuring out, is that it is much faster if I compress it to a zip file, upload it to google drive, and then access it from google colab. This ended up working very well, and I was able to unzip the file on Colab, for use. I started to copy over all the code I wrote from VSCode into Colab, and had to just make a few changes as I was not able to access my files from my computer anymore. I then quickly realized while trying to train the data, that I forgot to enable the gpu runtime, and I was running it the same as I was before. When I switched the runtime, it actually deleted my dataset, but this was not a hard fix, as I just unzipped the zip file yet again. Once I started to run the code to train the model, I was genuinely amazed at how fast it was. My old computer used to get through 100 training batches in about 10 minutes, and Colab was getting through them in about 30 seconds or less. Once again thanks to Sean for giving this suggestion. When my model finally trained, I received oddly great reesults. The accuracy of the model was about 94 - 98 percent, which seems too good to be true on my first attempt. Then after testing the model, It also did really well on the predictions, with images that the model has never seen before, with an accuracy of 99.1%. I'm not sure if my model is overfit or not, but I will be checking for that tomorrow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 4 (Thursday, June 9th, 2023): I presented my work so far, and got some good feedbaack. Seongha and I believe that my data is possibly too simple, and it's too easy for the mode to pickup, which wouldn't work out with a real world scenario input. So after the meeting, I started to reconstruct the data, the smoke stayed almost exactly the same, I just renamed the pictures to their correlating images. Now what I did differently, is the way I split the ground and sky. This time I am adding horizon, and using every single part of the pictures except for the smoke. I created a program which takes the picture, crops the smoke out and then saves the pictures. Then I created another program which takes those new pictures and splits them 3 ways, for the sky, horizon, and ground. There were about 8,300 pictures, so I have 8,300 pictures of each class. I also resized them in the same program, and accounted for smoke that may still be in the image. I hope I will have the cloud images by tomorrow, to add to the dataset as well. Either way I will be training a new VGG16 model, to see if this seems more accurate now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 5 (Friday, June 10th, 2023): After some thought last night, thee new dataset I created was not good. There are many instances of each fire, but the setting does not change throughout those instances, only the smoke does. The ground and sky is just the same for each, so the dataset I created yesterday had way too many duplicates. When I came in, I began to create the program for the new dataset. Smoke is not going to change as I believe it is already good enough. I decided to first do 3 rows and 4 columns. I compiled all the cropped and resized images, but it was far too little images. I kept thinking about it, and actually decided in the end to do 5 rows, and 6 columns. This way each full image(without the smoke) can be split into even tiles, which are then resized to 224x224. So this means that for every one setting, I get 12 sky images, 6 horizon images, and 12 ground images. There really isn't a good way to get 12 horizon images, as the key part of the image is the seperation of the ground and sky, so it should be alright with 6. Then once I had this dataset, I felt really good about how it turned out, so I went back to train the model. After trainining, I actually evaluated the model at a 90% accuracy, which is very good. It makes much more sense than the 98% model, and it is definitely because of the dataset improvement. After I did this model, I tested the model on predicting not everything, but just other and smoke. My first try training I once again got an oddly great model accuracy, but after further investigation I found that somehow all of my labeled sky images were actually ground images, so the data was heavily biased. I went back and fixed my mistake, and recieved a 96% accuracy on the testing data. Once again, I am not sure if my data is still too simple, but this can come to the test when I will test the model with random node images. My next steps include uploading my dataset to the lcrc, and also to test my dataset on the Smokeynet architecture from UCSD, to see if my data really is too simple, or if the simple VGG16 is simply just better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 3 (06/12 - 06/16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1 (Monday, June 12th, 2023): I worked remotely today as I had all the data and models on my personal laptop. Since I already had tested the VGG16 model on both other vs smoke and everything vs everything, I was ready to try and use the UCSD model with my data. After looking through their model, it was very confusing. All the files were seperated, and they would access eachother somehow, but I never saw code like that so I was puzzled. I spent the entire day playing around with it, but I couldn't seem to get it to work. I believe that it is because their model's code assumes its inputting a full raw image, which it then tries to split into tiles, whereas my data is already split into tiiles, so I don't need it to do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 2 (Tuesday, June 13th, 2023): I came into work today and started to try and work with the code. I showed Seongha how their code was organized, and she was puzzled as well. She looked into it and reccomended that I try and remove all of the code that splits the data into tiles, and to only use their model code, with my prior code to train and test. I began to try and make this work, and slowly started to get somehwere, making their code compatible with mine. I ran into a weird error which I just could not figure out. Their code was not making sense, and neither Sean nor Seongha could figure it out. Sean showed me how to use the vscode debugger, and we started to get somewhere."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 3 (Wednesday, June 14th, 2023): I kept on working on the vscode debugger which was pretty tough to work through. After much trial and error I was getting closer and closer to having the model work. I finally saw that when the tensor gets resized in the final layer of the model, the output was size 2. My data that I was using had 4 features, so it wasn't working when the model thought it only had 2. I played around with this and finally got it to work fine. I then saved my changes to the UCSD code, and implemented it on Colab in order to finally train and test my data. After training the model, I tested it and found a 92% accuracy on my data, which is almost the same as the accuracy from the VGG16 model. I then also trained the model on the other vs smoke dataset, with only 2 features. This also ended up giving about the same accuracy, just slighty worse than the VGG16, 96% compared to 97% accuracy on the testing data. With this being proven, I now know our model is pretty good and can continue with my research. The immediate next step is to add cloud images to the dataset, because it will definitely bring the accuracy down, and then we can see which model performs better overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 4 (Thursday, June 15th, 2023): After showing my results and progress in today's weekly meeting, I was given some tips and advice. I should have made the testing data solely events that aren't in the training nor evaluation, and to augment the images a little, which will result in more data. I created a program which takes every image in the ground dataset, and does a random 50/50 choice at if the image should be flipped or rotated, and then saves that new image which is augmented. I also continued this for sky, which resulted in about a doubling in data size. It isn't as necessary for sky, because most images are just a solid blue, but I did it anyways just to have somewhat even amounts of data for each feature. Horizon on the other hand, had far less data to begin with, so I rotated and flipped every image, and saved the two new images. This resulted in my tripling the horizon dataset, making it more on par with the other datasets. I then wanted to test the model on this new data, also by splitting the data smarter. I chose about 10 events that were not to be included in training nor evaluation,and then only included those events for testing. I did this by writing a program which would split everything still randomly, except for the testing data. I also randomly selected smoke, and only about half of it, so that it is even with the other datasets. I also kept the specific events seperate for the testing data. I uploaded this to google colab, and trained the model. This training took much longer as the dataset was much bigger. My results were very similar to before, a 92% accuracy, but this makes me more confident at the ability of this model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 5 (Friday, June 16th, 2023): I came in today to train the VGG16 and UCSD models. I trained the VGG16 yesterday, but I wanted to just make sure everything was accurate. I got about a 92% accuracy for both. Seongha shared with me about 20 cloud images to use in the dataset. Because of how little images there were, I had to augment the images in order to get some more. So I wrote a program which would take the images, flip them, rotate them, and then crop them in 4 pieces. This substantially increased the dataset, and then I went back through the new amount of images, and rotated them two more different ways which left me with 1500 images. I implemented this into the dataset and tried to train the VGG16 model, now with clouds. Without clouds, in the other vs smoke, the accuracy was 98%. After training it with clouds, so other vs clouds vs smoke, the accuracy was 92% on the testing data which was really good. I showed my results to Seongha, and she sent me another cloud dataset, to get even more cloud images. I started working on this dataset, and implementing it into the model yet again. Once I finally got everything ready, I now had a solid test train split with clouds as well. This came out to about 6,000 images for each feature. Now it was time to train. Once I trained the model, I got a 94% accuracy which is really good. The VGG16 does a great job at differentiating between smoke and cloud images. I showed my results and they were really good. I now trained the UCSD model with the fully completed dataset, and it only recieved about an 89% accuracy, which is much lower than the VGG16. This means a really good thing. When I come back to work after the weekend, I will be training 3 more models, the resnet34,50,101,152, and the the results of all those different models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 4 (06/19 - 06/23)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1 (Monday, June 19th, 2023): Worked on training all the other types of models. These models that I train will be Resnet18, Resnet34, and Resnet50. It took a while to go through and do all of this, but by the end I had got a good understanding. For the Resnet18 I recieved about a 90%, for the Resnet34 about a 94%, and for the Resnet50 about a 89%. The resnet34 performed about as well as the VGG16, and that is also the backbone model which was used by UCSD. With these results, I can now continue. The next step is to test the models with completely new images, and to see the False Positive Rate of smoke, whatever has the lowest FPR we can use in the nodes. This is important because we don't want any false alarms of forest fires. To do this I have to perform model inference on my model, and I am working on that currently. I have gotten it to work for single images at a time, but the results vary on the accuracy. I will need to do this more in depth tomorrow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
